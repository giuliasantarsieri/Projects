---
title: "Serie Chronologique"
author: "Eva BOGUSLAWSKI, Audrey CANAL, Yasmina Djerroud et Giulia SANTARSIERI"
date: "12 janvier 2021"
output: rmarkdown::github_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(lubridate)
library(zoo)
library(ggplot2)
library(tseries)
library(slider)
library(TSPred)
library(HarmonicRegression)
```

# Introduction aux données

Dans ce projet, on traitera un problème de séries temporelles sur le jeu de données *Household power consumption*, un dataset à 2,075,259 observations et 9 variables regroupant plusieurs mesures sur la consommation éléctrique d'un foyer pendant 4 ans (active power, reactive power, voltage etc. de Décembre 2006 à Novembre 2010). Pour notre étude nous avons décidé de se concentrer uniquement sur la variable *Global_House_Power* qui exprime la moyenne par minute de la consommation globale d'un foyer (cela correspond à l'active power qui est la partie réelle du complexe qui modélise le flux électrique). Nous allons essayer de mettre en oeuvre les méthodes abordées lors du cours de séries chronologiques dispensé par Monsieur JP. Baudry.

## Pre-traitement des données

Nous allons effectuer un pré-traitement des données en effectuant :
* une modification de la time zone
* une élimination de lignes contenant des valeurs manquantes (ici indiquées par "?")

Ici, on crée un sous-jeu de données qui sera l'objectif de notre étude: 
- on restraint notre jeu de données aux colonnes temporelles et à la colonne *Global_active_power* qu'on renommera *power*
- on crée des colonnes *year*, *month*, *day* et *int_hour* qui est un créneau horaire parmi 22h-6h, 7h-12h, 13h-17h et 18h-21h.

On agrège en faisant une moyenne *Global_active_power* par date et par créneau horaire. On obtient :

```{r ,warning=FALSE,message=FALSE}
powercons <- read.table("household_power_consumption.txt", header = TRUE, sep=";",dec = ".")
powercons_d <- powercons %>% mutate(DateTime = with_tz(dmy_hms(paste(powercons$Date, powercons$Time)), "Europe/Paris"), date=date(DateTime), Time_TZ=strftime(DateTime, format = "%H:%M:%S")) %>% distinct(date, Time_TZ, .keep_all = T)
powercons_d <- powercons_d[powercons_d$Global_active_power != "?", ]

build_creneau_horaire <- function(date){
    h_date <- hour(date)
    case_when(h_date %in% c(22,23,seq(0,6)) ~ "22-06",
          h_date %in% seq(7,12) ~ "07-12",
          h_date %in% seq(13,17) ~ "13-17",
          h_date %in% seq(18,21) ~ "18-21",)
}

data <- powercons_d %>% filter(Global_active_power != "?", DateTime >= "2007-01-01 00:00:00 CET")  %>% select(c(DateTime, date, Global_active_power)) %>% 
  mutate(Global_active_power = as.numeric(Global_active_power), int_hours = build_creneau_horaire(DateTime)) %>% 
  group_by(date, int_hours) %>% summarise(across(.fns = mean), .groups="drop") %>% 
  arrange(DateTime) %>% rename(Date = date) %>%
  mutate(year = year(DateTime), month = month(DateTime), day = day(DateTime), power = Global_active_power)
head(data)
```

## Visualisation de Global_active_power et Global_reative entre 2007 et 2010

On visualise nos données :

```{r , echo=F, warning=FALSE, message=FALSE}
ggplot(data, aes(x = DateTime, y = power)) +
  geom_line(color = "#69b3a2") + 
  xlab("Time") 
```

Nous constatons qu'il y a une saisonnalité mais qu'il n'y a pas forcément de tendance marquée.

Nous avons décidé de séparer nos données de sorte à garder la dernière semaine pour pouvoir évaluer nos prédictions. Puis on transforme nos données en série temporelle avec fréquence sur les créneaux horaires.

```{r}
new_power <- data$power[data$DateTime > "2010-11-19 19:59:30 CET"]
data_z <- data %>% filter(DateTime <= "2010-11-19 19:59:30 CET")
data_z$power <- ts(data_z$power, frequency=364*4,start=c(2007,1)) # on met 364 pour avoir un cycle sur les semaines (multiple de 7)
```

# Estimation de la tendance et de la saisonnalité

## Par régression harmonique

Nous allons effectuer une regression harmonique en utilisant la fonction `harmonic.regression`.

```{r}
Harmonic_reg1 <-harmonic.regression(as.vector(data_z$Global_active_power), 1:length(data_z$Global_active_power), Tau =364*4, normalize = TRUE,norm.pol = FALSE, norm.pol.degree = 1, trend.eliminate = TRUE,trend.degree = 1)
#test pour les autres périodes
Harmonic_reg2 <-harmonic.regression(as.vector(data_z$Global_active_power), 1:length(data_z$Global_active_power), Tau =7*4, normalize = TRUE,norm.pol = FALSE, norm.pol.degree = 1, trend.eliminate = TRUE,trend.degree = 1)
Harmonic_reg3 <-harmonic.regression(as.vector(data_z$Global_active_power), 1:length(data_z$Global_active_power), Tau =4, normalize = TRUE,norm.pol = FALSE, norm.pol.degree = 1, trend.eliminate = TRUE,trend.degree = 1)
```


On plot notre processus ainsi que résultat de la regression harmonique:


```{r}
plot(data_z$Global_active_power,type="l", ylab="power")
lines(ts(Harmonic_reg1$fit.vals), lwd=2, col="blue")
#lines(ts(Harmonic_reg2$fit.vals), lwd=2, col="green") # hebdomadaire
#lines(ts(Harmonic_reg3$fit.vals), lwd=2, col="orange") #quotidien #pas plot car cache le vert
```


On capture bien la saisonnalité annuelle avec cette fonction en choisissant le bon $\tau=364\times4$. Cependant la fonction ne permet pas de superposer plusieurs saisonnalités en même temps: on perd donc les saisonnalités hebdomadaire et quotidienne.

Maintenant, nous avons codé à la main la regression harmonique de sorte qu'elle prenne en compte la multi-périodicité de nos données.

```{r}
data_reg_harm = data
data_reg_harm["index"] = 1:nrow(data)
data_reg_harm["cos_ann"] = cos(1:nrow(data)*2*pi/364/4)
data_reg_harm["sin_ann"] = sin(1:nrow(data)*2*pi/364/4)
data_reg_harm["cos_hebd"] = cos(1:nrow(data)*2*pi/7/4)
data_reg_harm["sin_hebd"] = sin(1:nrow(data)*2*pi/7/4)
data_reg_harm["cos_day"] = cos(1:nrow(data)*2*pi/4)
data_reg_harm["sin_day"] = sin(1:nrow(data)*2*pi/4)
```

```{r}
test_lm <- lm(Global_active_power ~ cos_ann + sin_ann + cos_hebd + sin_hebd + cos_day + sin_day, data_reg_harm)
```

On plot le résultat:


```{r}
plot(data_z$Global_active_power, type = "l",ylab="power")
lines(ts(test_lm$fitted.values), col = "red")
```

On remarque qu'on retrouve la même allure que précédemment. Cependant, ici on constate une oscillation de la courbe qui traduit la prise en compte de la périodicité hebdomadaire et quotidienne: faisons un zoom sur un mois pour observer cette dernière.

```{r}
plot(data_z$Global_active_power[1:(28*4)], type = "l",ylab="power")
lines(ts(test_lm$fitted.values[1:(28*4)]), col = "red")
```

On constate qu'il prend en compte une saisonnalité journalière mais qu'il comprend peu la saisonnalité hebdomadaire. Par ailleurs on remarque que les vraies valeurs marquent des plus grands creux et piques. Peut-être qu'avec plus d'harmoniques, on s'approcherait encore plus du signal.



## Par moyennes glissantes

On suppose ici avoir un modèle du type: $\ X_{t} = \ m_{t}+ \ s_{t}+\ y_{t}$, avec $\ m_{t}$ la composante de tendance, $\ s_{t}$ la composante de saisonnalité et  $\ y_{t}$ un bruit aléatoire tel que $\mathbb{E}\ [\ y_{t}\ ] = 0$

On utilise ici la fonction `decompose`, qui dans un premier temps estime la tendance par moyenne glissante centrée (des poids tous égaux sont utilisés). La tendance est ensuite soustraite de la série et la saisonnalité est déterminée par une moyenne sur chaque unité de temps, sur toutes les périodes. Elle est enfin recentrée et soustraite de la série.

```{r}
power.dec <- decompose(data_z$power, type = "additive")
plot(power.dec)
```

Ce graphique décompose notre série en tendance, saisonnalité et bruit. Pour plus de clareté nous allons regarder seulement la saisonnalité et la tendance.

```{r}
trend <- power.dec$trend
season <- power.dec$seasonal
ts.plot(cbind(trend, season), lty = 1:2)
```

Le deuxième graphique nous permet bien de confirmer que nos données ne présentent pas de tendance et qu'on a une saisonnalité très marquée ayant une fréquence annuelle.

Il existe d'autres méthodes, un peu plus sophistiquées, telles que `stl`.

## Par différence

On utilise la fonction `diff`avec le paramètre `lag`.

```{r}
power.D <- diff(data_z$power, lag = 364*4) 
plot(power.D, type = "l")
```

On constate qu'on à éliminé la saisonnalité. De plus, on ne remarque pas de tendance donc nous n'avons pas besoin d'appliquer une méthode pour enlever la tendance (voir Annexe).

Finalement on choisira cette dernière méthode pour éliminer la saisonnalité. Ce choix est arbitraire parmi les méthodes présentées ci dessus. 

# Choix du modèle

Nous allons choisir le modèle qui va modéliser notre processus où la saisonnalité a été enlevée.

## Stationnarité

Pour appliquer notre modèle nous avons besoin de savoir si le processus est stationnaire : nous pouvons par exemple utiliser un des deux tests suivant pour savoir cela.

```{r}
adf.test(power.D) 
```

La p valeur est plus petite que 0.05 donc on rejette l'hypothèse nulle de non stationnarité.

```{r}
kpss.test(power.D) 
```

La p valeur est plus grande que 0,05 donc on ne rejette pas l'hypothèse nulle de stationnarité.

On peut conclure que notre processus transformé est stationnaire.

## ACF et PACF

Regardons l'ACF et le PACF.

```{r}
acf(power.D)
pacf(power.D)
```

Au vu de ces derniers nous émettons l'hypothèse que le modèle ARMA sera apaté ici.

## Modèle ARMA

Nous avons construit une fonction qui permette de faire une recherche exhaustive sur les paramètres du modèle ARMA (et de la fonction `arima`). Le critère de sélection sera l'AIC.

```{r, echo=T}
minimiserAIC = function(X,P,Q){ 
  p_min = P[1]
  q_min = Q[1]
  cpt = 0
  for (p in P[1]:P[2]){
    for (q in Q[1]:Q[2]){
      tryCatch({
        Mod = arima(X,order=c(p,0,q))
        aic = Mod$aic
      }, error=function(e){})
      if (cpt == 0){
        aic_min = aic
      }#end if
      else{
        if (aic < aic_min){
          aic_min = aic
          p_min = p
          q_min = q
        }#end if
      }#end else
      cpt = cpt+1
    }#end 2ème boucle
  }#end 1ere boucle
  return(c(p_min, 0, q_min))
}#endfunction
```

Nous avons exécuté cette fonction une fois avec les plages [8,15] pour l'ordre de l'AR et [6,10] pour l'ordre du MA. Pour déterminer ces plages nous nous sommes basées sur le PACF. Cette fonction ne sera pas évaluée dans le rapport car elle met du temps à tourner : c'est une recherche exhaustive et la fonction `arima` peut mettre du temps à tourner lorsque les valeurs des paramètres sont élevées.

```{r, eval =F}
MinAIC_X = minimiserAIC(power.D,c(8,15),c(6,10))
print(MinAIC_X)
```

Nous choisissons finalement de prendre un modèle ARMA(9,8) dont le résultat est affiché ci-dessous :

```{r}
res <- arima(power.D,order=c(9,0,8))
res
```

Un erreur se produit qui est dûe au fait que l'estimation des paramètres n'a pas pu aboutir car l'algorithme n'a pas convergé et a atteint le nombre maximal d'itération. Cela entraîne que certains paramètres ont une valeur `NA`. 

## Prédiction 

Nous allons donc prédire à l'aide de la fonction `predict` les 28 valeurs correspondantes à la dernière semaine de notre data-set originel. Une fois cela fait nous devrons nous ramener au processus de base.

```{r}
predres <- predict(res, n.ahead = 7*4) #prédiction avec le modèle choisit
Ytild <- predres$pred
XX <- data_z$power[data_z$DateTime <= "2009-11-27 19:59:30 CET" & data_z$DateTime > "2009-11-19 19:59:30 CET"]
Xtild<-c() #vecteur où on stockera les prédictions
for (i in 1:28){
  Xtild[i] <- Ytild[i] + XX[i]
}
```

Sur ce graphique vous pouvez voir en vert le processus réel, en noir pointillé la prédiction sur le processus transformé et en noir trait plein la prediction avec retour au processus de base.

```{r}
df_newpower <- data.frame("DateTime" = data$DateTime[data$DateTime > "2010-11-19 19:59:30 CET"],
                        "power" = new_power, "pred_Y" = Ytild, "pred" = Xtild)

ggplot(df_newpower, aes(x = DateTime, y = power)) +
  geom_line(color = "#69b3a2") + 
  geom_line(aes(y=pred_Y), linetype = "dashed") +
  geom_line(aes(y=pred)) +
  xlab("Time")
```

Le résultat de notre prédiction semble plutôt satisfaisant à l'oeil nu mis à part deux prédictions qui sont très éloignées des vraies valeurs . Pour être précis nous allons regarder l'erreur absolue moyenne en pourcentage (MAPE).

```{r}
MAPE<-mean(abs((Xtild-new_power)/new_power))*100
plot(abs((Xtild-new_power)/new_power),xlab="index",ylab=" Ratio erreur absolue")
abline(h=median(abs((Xtild-new_power)/new_power)),col="#69b3a2",lwd=2)
legend(20,8,legend=c("mediane"),col=c("#69b3a2"),lwd=2)
```

- On obtient un MAPE de `r round(MAPE,2)` ce qui n'est pas exceptionnel.
- On remarque que l'index numéro 8 a une grande valeur. Comme certaines erreurs extrêmes tirent le **MAPE** vers le haut, on regarde la médiane.
- La médiane est de `r round(median(abs((Xtild-new_power)/new_power)),2)` ce qui signifie qu'on a environ `r round(median(abs((Xtild-new_power)/new_power))*100)`%  d'erreur par prédiction.

Même en se basant sur la médiane on constate de mauvaises performances. Cela est peut-être lié au fait que plusieurs saisonnalités différentes se superposent : annuelles, hebdomadaires, quotidiennes. De plus, les paramètres d'ARMA ne sont peut être pas optimal car nous n'avons pas testé toutes les combinaisons de paramètres possibles et lors de l'exécution nous avons un problème de convergence.  

# Conclusion

Nous avons pu aborder plusieurs sujets dans ce compte rendu de projet. En effet, nous avons pu mettre en oeuvre des méthodes d'élimination de saisonnalités (et tendance, cf Annexe) ainsi que l'utilisation du modèle ARMA pour la modélisation de séries temporelles. Malgré nos résultats peu concluants en prédiction dus à des problèmes cités précédemment nous sommes néanmoins maintenant plus habiles avec l'utilisation de ces méthodes.

![](meme.JPG)

# Annexe : estimation de la tendance

Dans le cadre de notre étude l'estimation de la tendance n'est pas nécessaire. Cependant, comme l'objectif est d'apprendre à mettre en oeuvre des méthodes vues en cours nous avons décidé de regarder l'implémentation de méthodes d'élimination de la tendance. Ces dernières sont présentés dans cette Annexe.


## Smoothing with a finite moving average filter

```{r,echo=FALSE}
test <- slide_dbl(data_z$power, ~mean(.x), .before = 7*4, .after =7*4)
ggplot(data_z, aes(x=Date, y=power)) +
  geom_line(size = 1, color = "red")+
  geom_line(aes(y=test)) +
  xlab("")
```


On capture ici l'allure de notre processus: cela correspond ici à une partie de la saisonnalité et non la tendance car il n'y en a pas. Le résultat reste néanmoins convaincant.


## Exponential smoothing: méthode de Holt-Winters

Le lissage exponentiel fait partie des techniques d'estimation de la tendance.
En général, le lissage exponentiel se base sur le modèle suivant:

$\ X_{t} = \ m_{t}+\ y_{t}$, avec $\ m_{t}$ la composante de tendance et  $\ y_{t}$ un bruit aléatoire tel que $\mathbb{E}\ [\ y_{t}\ ] = 0$. On suppose qu'il y a absence de saisonnalité.

L'estimation de la moyenne $\ m_{t}$ dans le lissage exponentiel est alors égale à $\ m_{1} = \ X_{1}$, $\ m_{t} = \alpha \ X_{t} + (1-\alpha)m_{t-1}$, $\alpha \in [0,1]$.

Voici le $\alpha$ optimal estimé par la fontion `HoltWinters` en minimisant l'erreur SS1PE (squared one-step-ahead prediction errors):

```{r}
exp.smooth <- HoltWinters(data_z$power,beta=FALSE,gamma=FALSE)
exp.smooth$alpha
```

Et voici l'estimation de la tendance :

```{r}
plot(exp.smooth)
plot(exp.smooth$fitted)
```


Cette méthode n'est pas très adaptée à nos données, car elles présentent très clairement une saisonnalité. 

On ne pourra donc pas faire l'hypothèse d'absence de saisonnalité. On utilise alors la méthode de Holt-Winters de lissage exponentiel, qui prend en compte la présence de saisonnalité à l'aide des paramètres $\beta$ et $\gamma$.

Voici l'estimation de la tendance par cette méthode:

```{r}
holt.smooth <- HoltWinters(data_z$power)
plot(holt.smooth$fitted)
#holt.smooth$coefficients[2]
```

Les résultats sont plus satisfaisants en prenant en compte la saisonnalité.

```{r}
#holt.smooth$alpha
#holt.smooth$beta
#holt.smooth$gamma
```

## Polynomial fitting

On teste toutes les combinaisons linéaires de monômes de degré inférieur ou égal à 5.
 
Voici le modèle choisi par MSE:



```{r}
fPolyR<-fittestPolyR(data_z$Global_active_power,new_power,h=28,rank.by=c("MSE"))
trend <- fitted(fPolyR$model)
residuals <- detrend(data_z$Global_active_power,trend)
x <- detrend.rev(residuals,trend)
pred <- fPolyR$pred
fPolyR$model
```


Visualisons la prédiction avec ce modèle: 

```{r}
plot(c(data_z$Global_active_power,new_power),type='o',lwd=2,xlim=c(5610,5644),ylim=c(0,5),
xlab="Time",ylab="Predicted")
#plotting predicted values
lines(ts(pred$mean,start=5617),lwd=2,col='blue')
#plotting prediction intervals
lines(ts(pred$lower,start=5617),lwd=2,col='light blue')
lines(ts(pred$upper,start=5617),lwd=2,col='light blue')
```


On constate que les prédictions ne sont pas du tout bonnes. En effet la droite bleue ne suit pas du tout la courbe des vraies valeurs.



